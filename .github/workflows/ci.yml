# CI/CD Pipeline for pmake-recover
# Comprehensive testing with security focus and 100% coverage requirement

name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:  # Allow manual triggers
  schedule:
    # Run security checks weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'

env:
  PYTHON_VERSION: '3.12'
  COVERAGE_THRESHOLD: 100

jobs:
  # Security scanning - runs first to catch issues early
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for security analysis

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-security-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-security-
            ${{ runner.os }}-pip-

      - name: Install security scanning dependencies
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety semgrep

      - name: Run Bandit security linter
        run: |
          bandit -r . -f json -o bandit-report.json || true
          bandit -r . -f txt -o bandit-report.txt || true
          # Continue even if issues found - we'll report them

      - name: Run Safety dependency check
        run: |
          safety check --json --output safety-report.json || true
          safety check --output safety-report.txt || true

      - name: Run Semgrep security analysis
        run: |
          semgrep --config=auto --json --output=semgrep-report.json . || true
          semgrep --config=auto --output=semgrep-report.txt . || true

      - name: Upload security reports
        uses: actions/upload-artifact@v3
        with:
          name: security-reports
          path: |
            bandit-report.*
            safety-report.*
            semgrep-report.*
          retention-days: 30

  # Code quality and linting
  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-quality-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-quality-
            ${{ runner.os }}-pip-

      - name: Install development dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Run Black formatter check
        run: |
          black --check --diff --color .

      - name: Run isort import sorting check
        run: |
          isort --check-only --diff --color .

      - name: Run flake8 linting
        run: |
          flake8 . --statistics --tee --output-file=flake8-report.txt

      - name: Run pylint analysis
        run: |
          pylint --output-format=json:pylint-report.json,text:pylint-report.txt *.py || true

      - name: Run mypy type checking
        run: |
          mypy --install-types --non-interactive . || true

      - name: Upload quality reports
        uses: actions/upload-artifact@v3
        with:
          name: quality-reports
          path: |
            flake8-report.txt
            pylint-report.*
          retention-days: 30

  # Main test suite - runs in parallel matrix
  test:
    name: Test Suite
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.11', '3.12']
        
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-${{ matrix.python-version }}-pip-
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Create test directories
        run: |
          mkdir -p test-results reports

      - name: Run unit tests with coverage
        run: |
          pytest tests/unit/ \
            --verbose \
            --cov=. \
            --cov-report=xml:reports/coverage-unit.xml \
            --cov-report=html:reports/htmlcov-unit \
            --cov-report=term-missing \
            --junit-xml=test-results/junit-unit.xml \
            --html=test-results/report-unit.html \
            --json-report --json-report-file=test-results/report-unit.json \
            --benchmark-json=test-results/benchmark-unit.json

      - name: Run integration tests with coverage
        run: |
          pytest tests/integration/ \
            --verbose \
            --cov=. \
            --cov-append \
            --cov-report=xml:reports/coverage-integration.xml \
            --cov-report=html:reports/htmlcov-integration \
            --cov-report=term-missing \
            --junit-xml=test-results/junit-integration.xml \
            --html=test-results/report-integration.html \
            --json-report --json-report-file=test-results/report-integration.json \
            --benchmark-json=test-results/benchmark-integration.json

      - name: Run security tests
        run: |
          pytest tests/ -m security \
            --verbose \
            --junit-xml=test-results/junit-security.xml \
            --html=test-results/report-security.html \
            --json-report --json-report-file=test-results/report-security.json

      - name: Generate combined coverage report
        run: |
          coverage combine
          coverage report --fail-under=${{ env.COVERAGE_THRESHOLD }}
          coverage xml -o reports/coverage-combined.xml
          coverage html -d reports/htmlcov-combined
          coverage json -o reports/coverage-combined.json

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}
          path: |
            test-results/
            reports/
          retention-days: 30

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.12'
        with:
          file: reports/coverage-combined.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: true

  # Performance benchmarking
  performance:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [test]
    if: github.event_name != 'schedule'  # Skip on scheduled runs
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run performance benchmarks
        run: |
          pytest tests/ -m slow \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --benchmark-histogram=benchmark-histogram

      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        if: github.ref == 'refs/heads/main'
        with:
          tool: 'pytest'
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          comment-on-alert: true
          alert-threshold: '150%'

  # Package building and validation
  build:
    name: Build Package
    runs-on: ubuntu-latest
    needs: [security-scan, code-quality, test]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install build dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build twine check-manifest

      - name: Check manifest
        run: check-manifest

      - name: Build package
        run: python -m build

      - name: Check package
        run: twine check dist/*

      - name: Upload build artifacts
        uses: actions/upload-artifact@v3
        with:
          name: package-builds
          path: dist/
          retention-days: 30

  # Final validation and reporting
  validate:
    name: Final Validation
    runs-on: ubuntu-latest
    needs: [security-scan, code-quality, test, performance, build]
    if: always()
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v3

      - name: Generate final report
        run: |
          echo "## CI/CD Pipeline Summary" > final-report.md
          echo "- Security Scan: ${{ needs.security-scan.result }}" >> final-report.md
          echo "- Code Quality: ${{ needs.code-quality.result }}" >> final-report.md
          echo "- Tests: ${{ needs.test.result }}" >> final-report.md
          echo "- Performance: ${{ needs.performance.result }}" >> final-report.md
          echo "- Build: ${{ needs.build.result }}" >> final-report.md
          
          # Check if any critical jobs failed
          if [[ "${{ needs.security-scan.result }}" == "failure" || \
                "${{ needs.test.result }}" == "failure" ]]; then
            echo "❌ Critical failures detected!" >> final-report.md
            exit 1
          fi
          
          echo "✅ All critical checks passed!" >> final-report.md

      - name: Upload final report
        uses: actions/upload-artifact@v3
        with:
          name: final-report
          path: final-report.md
          retention-days: 90